# GitHub Hub - AI åˆ†æ Agent
from openai import OpenAI
from typing import Dict, List, Optional
import json
from config import LM_STUDIO_BASE, LM_STUDIO_KEY, MODELS

class AnalyzerAgent:
    """AI åˆ†æ Agent - ä½¿ç”¨æœ¬åœ° LM Studio æ¨¡å‹"""
    
    def __init__(self):
        try:
            self.client = OpenAI(base_url=LM_STUDIO_BASE, api_key=LM_STUDIO_KEY)
        except Exception as e:
            print(f"Warning: Could not initialize OpenAI client: {e}")
            self.client = None
    
    def analyze_project(self, project: Dict, readme: Optional[str] = None) -> Dict:
        """åˆ†æå•ä¸ªé¡¹ç›®ï¼Œç”Ÿæˆ Metadata"""
        
        context = f"""
é¡¹ç›®åç§°: {project['name']}
å®Œæ•´åç§°: {project['full_name']}
æè¿°: {project.get('description', 'æ— ')}
è¯­è¨€: {project.get('language', 'æœªçŸ¥')}
æ˜Ÿæ ‡: {project['stars']}
Topics: {', '.join(project.get('topics', []))}
"""
        if readme:
            context += f"\n\nREADME å†…å®¹ (å‰3000å­—):\n{readme[:3000]}"
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹ GitHub å¼€æºé¡¹ç›®ï¼Œå¹¶ä»¥ JSON æ ¼å¼è¿”å›åˆ†æç»“æœï¼š

{context}

è¯·è¿”å›ä»¥ä¸‹ JSON æ ¼å¼ï¼ˆç¡®ä¿æ˜¯æœ‰æ•ˆçš„ JSONï¼‰ï¼š
{{
    "summary": "ä¸€å¥è¯ç²¾å‡†æ¦‚æ‹¬ï¼šè¿™æ˜¯åšä»€ä¹ˆçš„+æ ¸å¿ƒæŠ€æœ¯ç‰¹ç‚¹+ç‹¬ç‰¹ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼š'åŸºäºLangChainçš„å¤šAgentå¯¹è¯ç³»ç»Ÿï¼Œæ”¯æŒæ’ä»¶å¼å·¥å…·è°ƒç”¨å’Œé•¿æœŸè®°å¿†'",
    "tech_stack": ["æ ¸å¿ƒæŠ€æœ¯æ ˆï¼Œå¦‚ LangChain, FastAPI, ChromaDB ç­‰"],
    "use_cases": ["å…·ä½“åº”ç”¨åœºæ™¯1", "åœºæ™¯2", "åœºæ™¯3"],
    "difficulty": 1-5 çš„æ•°å­—,
    "highlights": ["æŠ€æœ¯äº®ç‚¹1", "äº®ç‚¹2"],
    "ai_tags": ["3-5ä¸ªæœ€èƒ½æè¿°é¡¹ç›®åŠŸèƒ½ç‰¹ç‚¹çš„å…³é”®è¯ï¼Œå¦‚ 'å¤šæ¨¡æ€', 'RAGæ£€ç´¢', 'æµå¼å¯¹è¯', 'ä»£ç ç”Ÿæˆ', 'çŸ¥è¯†åº“' - ä¸è¦ç”¨å¤ªå®½æ³›çš„è¯å¦‚ 'AI', 'Python'"],
    "quick_start": "å¿«é€Ÿå¯åŠ¨å‘½ä»¤"
}}

åªè¿”å› JSONï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ã€‚"""

        try:
            if not self.client:
                return self._default_analysis(project)
            response = self.client.chat.completions.create(
                model=MODELS["classifier"],  # ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1000
            )
            
            content = response.choices[0].message.content.strip()
            # æå– JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            return json.loads(content)
            
        except json.JSONDecodeError as e:
            print(f"[Analyzer] JSON parse error for {project['name']}: {e}")
            return self._default_analysis(project)
        except Exception as e:
            print(f"[Analyzer] Error analyzing {project['name']}: {e}")
            return self._default_analysis(project)
    
    def generate_rag_summary(self, project: Dict, readme: Optional[str] = None) -> str:
        """ç”Ÿæˆé¢å‘ RAG çš„é«˜å¯†åº¦æ€»ç»“ (ç»™ LLM çœ‹çš„)"""
        context = f"é¡¹ç›®: {project['full_name']}\næè¿°: {project.get('description', '')}\nTopics: {project.get('topics', [])}"
        if readme:
            context += f"\nREADME extract: {readme[:2000]}"
            
        prompt = f"""è¯·ä¸ºä»¥ä¸‹ GitHub é¡¹ç›®ç”Ÿæˆä¸€æ®µ**é¢å‘ LLM çš„é«˜å¯†åº¦æ€»ç»“** (RAG Summary)ã€‚
ç›®æ ‡ï¼šå½“ç”¨æˆ·æé—®æ—¶ï¼Œæœç´¢å¼•æ“å¯ä»¥é€šè¿‡è¿™æ®µæ€»ç»“å¿«é€Ÿåˆ¤æ–­è¯¥é¡¹ç›®æ˜¯å¦ç¬¦åˆç”¨æˆ·éœ€æ±‚ã€‚

{context}

è¦æ±‚ï¼š
1. **é«˜åº¦æµ“ç¼©**ï¼šä¸è¦åºŸè¯ï¼Œç›´æ¥åˆ—å‡ºæ ¸å¿ƒåŠŸèƒ½ã€‚
2. **åœºæ™¯å¯¼å‘**ï¼šæ˜ç¡®æŒ‡å‡º"é€‚åˆåšä»€ä¹ˆ" (Ideal for...)ã€‚
3. **æŠ€æœ¯å…³é”®è¯**ï¼šåŒ…å«å…³é”®æ¶æ„ã€ä¾èµ–åº“ã€ç®—æ³•åè¯ã€‚
4. **ä¼˜ç¼ºç‚¹**ï¼šç®€è¦æåŠæ½œåœ¨é™åˆ¶ã€‚
5. **æ ¼å¼**ï¼šçº¯æ–‡æœ¬ï¼Œä¸è¦ Markdown æ ¼å¼ï¼Œæ§åˆ¶åœ¨ 150 å­—ä»¥å†…ã€‚

ç¤ºä¾‹æ ¼å¼ï¼š
"[Python] åŸºäºFastAPIçš„å¼‚æ­¥Webæ¡†æ¶ã€‚æ ¸å¿ƒä¼˜åŠ¿æ˜¯é«˜æ€§èƒ½å’Œè‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆã€‚é€‚åˆæ„å»ºé«˜å¹¶å‘å¾®æœåŠ¡ã€‚ä¾èµ–uvicornã€‚ä¸æ”¯æŒPython 3.6ä»¥ä¸‹ã€‚ç±»ä¼¼Flaskä½†æ›´å¿«ã€‚"
"""
        try:
            response = self.client.chat.completions.create(
                model=MODELS["classifier"], # Use fast model
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=300
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"[Analyzer] RAG summary failed: {e}")
            return project.get('description', '')
    
    def _default_analysis(self, project: Dict) -> Dict:
        """é»˜è®¤åˆ†æç»“æœ"""
        return {
            "summary": project.get('description', 'æš‚æ— æè¿°'),
            "tech_stack": [project.get('language', 'Unknown')],
            "use_cases": ["é€šç”¨å¼€å‘"],
            "difficulty": 3,
            "quick_start": f"git clone {project['url']}"
        }
    
    def classify_project(self, project: Dict, categories: List[str]) -> str:
        """è‡ªåŠ¨åˆ†ç±»é¡¹ç›®"""
        prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹é¡¹ç›®æœ€é€‚åˆå“ªä¸ªåˆ†ç±»ï¼š

é¡¹ç›®: {project['name']}
æè¿°: {project.get('description', 'æ— ')}
è¯­è¨€: {project.get('language', 'æœªçŸ¥')}
Topics: {', '.join(project.get('topics', []))}

å¯é€‰åˆ†ç±»: {', '.join(categories)}

åªè¿”å›ä¸€ä¸ªåˆ†ç±»åç§°ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=MODELS["classifier"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=50
            )
            return response.choices[0].message.content.strip()
        except:
            return "awesome"  # é»˜è®¤å½’ç±»åˆ°å­¦ä¹ èµ„æº

    def refine_search_intent(self, history: List[Dict]) -> Dict:
        """æ ¹æ®å¯¹è¯å†å²ä¼˜åŒ–æœç´¢æ„å›¾"""
        
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        conversation = ""
        for msg in history[-5:]: # å–æœ€è¿‘5è½®
            role = "User" if msg['role'] == 'user' else "Assistant"
            conversation += f"{role}: {msg['content']}\n"
            
        prompt = f"""ä½ æ˜¯ä¸€ä¸ª**ä¸¥è°¨çš„éœ€æ±‚åˆ†æå¸ˆ**ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¾…åŠ©ç”¨æˆ·ç²¾ç¡®å®šä¹‰ GitHub æœç´¢éœ€æ±‚ã€‚

**æ ¸å¿ƒåŸåˆ™ï¼šç»ä¸è½»æ˜“æœç´¢**ã€‚
ç»å¤§å¤šæ•°ç”¨æˆ·çš„åˆå§‹æè¿°éƒ½æ˜¯æ¨¡ç³Šçš„ï¼ˆä¾‹å¦‚ "æ‰¾ä¸ªçˆ¬è™«"ï¼‰ã€‚ä½ çš„å·¥ä½œæ˜¯åƒå¹¶åœ¨**å¼€å§‹æœç´¢å‰**ï¼Œå¿…é¡»æ”¶é›†é½ä»¥ä¸‹ **3 ä¸ªå…³é”®è¦ç´ **ï¼š

1. **æ ¸å¿ƒæŠ€æœ¯æ ˆ (Tech Stack)**: å¿…é¡»æ˜ç¡®è¯­è¨€ï¼ˆå¦‚ Python/Go/Rustï¼‰æˆ–ç‰¹å®šæ¡†æ¶ï¼ˆå¦‚ React/Next.jsï¼‰ã€‚
2. **åº”ç”¨åœºæ™¯ (Context)**: æ˜¯è¦ä¸€ä¸ª**å¼€ç®±å³ç”¨çš„å·¥å…· (Application)**ï¼Œè¿˜æ˜¯**å¼€å‘è€…åº“ (Library/SDK)**ï¼Ÿæˆ–æ˜¯**å­¦ä¹ æ•™ç¨‹ (Tutorial)**ï¼Ÿ
3. **å…³é”®ç‰¹æ€§ (Key Features)**: å…·ä½“çš„å·®å¼‚åŒ–éœ€æ±‚ï¼ˆä¾‹å¦‚ "æ”¯æŒåˆ†å¸ƒå¼"ã€"å¸¦æœ‰ Web UI"ã€"è½»é‡çº§"ï¼‰ã€‚

---
**å†³ç­–æµç¨‹**:

**æƒ…å†µ A: ä¿¡æ¯ç¼ºå¤± (Action: question)**
å¦‚æœä¸Šè¿° 3 ä¸ªè¦ç´ ä¸­æœ‰**ä»»æ„ä¸€ä¸ª**ä¸æ˜ç¡®ï¼š
- **å¿…é¡»**æå‡ºé’ˆå¯¹æ€§çš„é—®é¢˜ã€‚
- ä¸€æ¬¡åªé—® 1-2 ä¸ªæœ€é‡è¦çš„é—®é¢˜ï¼Œä¸è¦ä¸€æ¬¡æ€§æŠ›å‡ºå¤ªå¤šã€‚
- è¯­æ°”è¦ä¸“ä¸šã€å¼•å¯¼æ€§å¼ºã€‚

**æƒ…å†µ B: ä¿¡æ¯å®Œå¤‡ (Action: search)**
åªæœ‰å½“ç”¨æˆ·å·²ç»æ˜ç¡®æä¾›äº†æ‰€æœ‰è¦ç´ ï¼Œæˆ–è€…ç”¨æˆ·æ˜¾å¼è¦æ±‚ "ç›´æ¥æœ" æ—¶ï¼š
- ç”Ÿæˆç²¾å‡†çš„ GitHub Search Queryã€‚
- å¿…é¡»åŒ…å« `language:` å’Œ `topic:` è¿‡æ»¤å™¨ã€‚

---
**ç¤ºä¾‹**:

**åœºæ™¯ 1 (ä¿¡æ¯æç¼º)**
User: "æƒ³è¦ä¸ªåå°ç®¡ç†"
Assistant: JSON {{ "action": "question", "content": "åå°ç®¡ç†ç³»ç»Ÿæœ‰å¾ˆå¤šç§ã€‚è¯·é—®æ‚¨æ˜¯åå¥½ **Java (Spring Boot)**, **Python (Django/FastAPI)** è¿˜æ˜¯ **Node.js**ï¼Ÿå¦å¤–ï¼Œæ‚¨æ˜¯éœ€è¦ä¸€å¥—**ç°æˆçš„ Admin UI** è¿˜æ˜¯è¦ä¸€ä¸ª**åç«¯æ¡†æ¶**ï¼Ÿ", "reasoning": "ç¼ºå°‘æŠ€æœ¯æ ˆå’Œç±»å‹" }}

**åœºæ™¯ 2 (ç¼ºå°‘åœºæ™¯)**
User: "Python çˆ¬è™«"
Assistant: JSON {{ "action": "question", "content": "æ”¶åˆ°ã€‚è¯·é—®æ‚¨æ˜¯æƒ³æ‰¾ä¸€ä¸ª**å¼€ç®±å³ç”¨çš„çˆ¬è™«è½¯ä»¶**ï¼ˆå¦‚ç›´æ¥çˆ¬å„ç§ç½‘ç«™çš„å·¥å…·ï¼‰ï¼Œè¿˜æ˜¯éœ€è¦ä¸€ä¸ª**Python å¼€å‘åº“**ï¼ˆå¦‚ Scrapy/Playwrightï¼‰æ¥è‡ªå·±å†™ä»£ç ï¼Ÿ", "reasoning": "ä¸ç¡®å®šæ˜¯å·¥å…·è¿˜æ˜¯åº“" }}

**åœºæ™¯ 3 (ç¼ºå°‘ç‰¹æ€§)**
User: "Python å†™çš„çˆ¬è™«åº“ï¼Œç”¨æ¥çˆ¬æ–°é—»"
Assistant: JSON {{ "action": "question", "content": "äº†è§£ã€‚é’ˆå¯¹æ–°é—»çˆ¬å–ï¼Œæ‚¨æ˜¯å¦éœ€è¦æ”¯æŒ**å¼‚æ­¥ (Async)**ï¼Ÿæˆ–è€…æ˜¯å¦éœ€è¦**è‡ªåŠ¨å¤„ç†åçˆ¬è™« (Anti-detect)** çš„åŠŸèƒ½ï¼Ÿ", "reasoning": "å°è¯•æŒ–æ˜é«˜çº§ç‰¹æ€§" }}

**åœºæ™¯ 4 (å®Œå¤‡)**
User: "Python å¼‚æ­¥æ–°é—»çˆ¬è™«ï¼Œæ”¯æŒåçˆ¬"
Assistant: JSON {{ "action": "search", "content": "news crawler language:python topic:asyncio topic:anti-detect pushed:>2023-01-01", "reasoning": "è¦ç´ é½å¤‡" }}

è¯·åˆ†æä»¥ä¸‹å¯¹è¯ï¼Œè¿”å› JSON:"""

        try:
            response = self.client.chat.completions.create(
                model=MODELS["classifier"], # ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=200
            )
            content = response.choices[0].message.content.strip()
            # Clean JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
                
            return json.loads(content)
        except Exception as e:
            print(f"[Analyzer] Refine intent failed: {e}")
            # Fallback to search if error
            last_user_msg = next((m['content'] for m in reversed(history) if m['role'] == 'user'), "")
            return {"action": "search", "content": last_user_msg, "reasoning": "Fallback"}

    def analyze_with_vision(self, project: Dict, image_path: str) -> str:
        """ä½¿ç”¨è§†è§‰æ¨¡å‹åˆ†ææˆªå›¾ (OCR & UIç†è§£)"""
        import base64
        
        try:
            with open(image_path, "rb") as image_file:
                base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                
            prompt = f"""è¯·ä»”ç»†è§‚å¯Ÿè¿™å¼  GitHub é¡¹ç›®çš„æˆªå›¾ (OCR)ï¼š
            
é¡¹ç›®: {project['name']}
æè¿°: {project.get('description', '')}

è¯·æå–å›¾ä¸­çš„å…³é”®æ–‡æœ¬ä¿¡æ¯ï¼Œå¹¶åˆ†æ UI/ç•Œé¢ è®¾è®¡é£æ ¼ã€‚
é‡ç‚¹å…³æ³¨ï¼š
1. ç•Œé¢ä¸»è¦åŠŸèƒ½æ¨¡å—
2. ä»»ä½•å¯è§çš„æŠ€æœ¯å…³é”®è¯
3. è¿™ä¸ªåº”ç”¨çœ‹èµ·æ¥æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ

è¯·ç”¨ä¸€æ®µç®€çŸ­çš„ä¸­æ–‡æ€»ç»“ä½ çš„è§†è§‰åˆ†æç»“æœã€‚"""

            response = self.client.chat.completions.create(
                model=MODELS["vision"], 
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=500
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"[Analyzer] Vision analysis failed: {e}")
            return "è§†è§‰åˆ†æå¤±è´¥"


class ContentAgent:
    """å†…å®¹ç”Ÿæˆ Agent - ç”Ÿæˆæ•™ç¨‹å’Œæ–‡æ¡£"""
    
    def __init__(self):
        try:
            self.client = OpenAI(base_url=LM_STUDIO_BASE, api_key=LM_STUDIO_KEY)
        except Exception as e:
            print(f"Warning: Could not initialize ContentAgent OpenAI client: {e}")
            self.client = None
    
    
    def generate_tutorial(self, project: Dict, readme: Optional[str] = None, visual_summary: str = "") -> str:
        """ç”Ÿæˆé¡¹ç›®æ•™ç¨‹ (ä½¿ç”¨å¼ºåŠ›æ¨¡å‹)"""
        
        context = f"""
é¡¹ç›®: {project['full_name']}
æè¿°: {project.get('description', '')}
è¯­è¨€: {project.get('language', '')}
æ˜Ÿæ ‡: {project['stars']}
"""
        if readme:
            context += f"\nREADME (éƒ¨åˆ†):\n{readme[:4000]}"
            
        if visual_summary:
            context += f"\n\nUI/ç•Œé¢è§†è§‰åˆ†æ (OCR):\n{visual_summary}"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±æ¶æ„å¸ˆã€‚ç›®æ ‡è¯»è€…æ˜¯**æœ‰ç»éªŒçš„å¼€å‘è€…**ï¼Œè¯·è·³è¿‡åŸºç¡€æ¦‚å¿µè§£é‡Šï¼ˆå¦‚"ä»€ä¹ˆæ˜¯AI Agent"ã€"ä»€ä¹ˆæ˜¯RAG"ç­‰ï¼‰ã€‚ç›´æ¥åˆ†æè¿™ä¸ªé¡¹ç›®çš„**ç‹¬ç‰¹ä»·å€¼å’ŒæŠ€æœ¯å®ç°ç»†èŠ‚**ã€‚

{context}

CRITICAL: ç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼Œä¸è¦æœ‰ä»»ä½•å¼€åœºç™½ï¼ˆå¦‚â€œå½“ç„¶å¯ä»¥â€ã€â€œå¥½çš„â€ã€â€œè¿™æ˜¯ä¸ºæ‚¨ç”Ÿæˆçš„...â€ç­‰ï¼‰ï¼Œä¹Ÿä¸è¦æœ‰ç»“æŸè¯­ã€‚ç›´æ¥å¼€å§‹æ•™ç¨‹å†…å®¹ã€‚

## ğŸ“š æ•™ç¨‹ç»“æ„è¦æ±‚ï¼ˆè¯·ä¸¥æ ¼æŒ‰æ­¤æ ¼å¼è¾“å‡ºï¼‰ï¼š

### ğŸ¯ æ ¸å¿ƒä»·å€¼ & å·®å¼‚åŒ–
(è¿™ä¸ªé¡¹ç›®å’ŒåŒç±»é¡¹ç›®ç›¸æ¯”ï¼Œ**ç‹¬ç‰¹åœ¨å“ªé‡Œ**ï¼Ÿè§£å†³äº†ä»€ä¹ˆåˆ«äººæ²¡è§£å†³å¥½çš„é—®é¢˜ï¼Ÿ)

### ğŸ”¥ æŠ€æœ¯äº®ç‚¹
(è¿™ä¸ªé¡¹ç›®æœ‰ä»€ä¹ˆ**å€¼å¾—å­¦ä¹ çš„æŠ€æœ¯å®ç°**ï¼Ÿæ¯”å¦‚ï¼š
- æŸä¸ªå·§å¦™çš„ç®—æ³•è®¾è®¡
- ç‹¬ç‰¹çš„æ¶æ„é€‰æ‹©
- æ€§èƒ½ä¼˜åŒ–æ‰‹æ®µ)

### ğŸ—ï¸ æ¶æ„è®¾è®¡åˆ†æ
(è¿™æ˜¯**è¿›é˜¶**å†…å®¹ï¼Œè¯·åˆ†æï¼š
1. é¡¹ç›®çš„æ•´ä½“æ¶æ„å›¾ï¼ˆç”¨æ–‡å­—æè¿°æˆ– ASCII å›¾ï¼‰
2. æ ¸å¿ƒæ¨¡å—åˆ’åˆ†å’ŒèŒè´£
3. æ•°æ®æµå‘ï¼ˆè¾“å…¥ â†’ å¤„ç† â†’ è¾“å‡ºï¼‰
4. å…³é”®è®¾è®¡æ¨¡å¼ï¼šç”¨äº†ä»€ä¹ˆè®¾è®¡æ¨¡å¼ï¼Ÿä¸ºä»€ä¹ˆè¿™ä¹ˆè®¾è®¡ï¼Ÿ)

### ğŸ”§ æŠ€æœ¯æ ˆæ·±åº¦è§£æ
(ä¸åªæ˜¯åˆ—å‡ºæŠ€æœ¯æ ˆï¼Œè¦è§£é‡Šï¼š
- ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæŠ€æœ¯ï¼Ÿæœ‰ä»€ä¹ˆæ›¿ä»£æ–¹æ¡ˆï¼Ÿ
- æ ¸å¿ƒä¾èµ–åº“çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
- ç‰ˆæœ¬å…¼å®¹æ€§æ³¨æ„äº‹é¡¹)

### ğŸ“¦ å®‰è£…ä¸é…ç½®
(æä¾›å¤åˆ¶ç²˜è´´çš„å‘½ä»¤ + æ¯æ­¥æ³¨é‡Š)

### ğŸ® ä½¿ç”¨ç¤ºä¾‹
(æä¾›ä¸€ä¸ª**å®Œæ•´çš„ä»£ç ç¤ºä¾‹**ï¼ŒåŒ…å«ï¼š
- çœŸå®åœºæ™¯çš„è¾“å…¥
- é¢„æœŸè¾“å‡º
- å…³é”®å‚æ•°è¯´æ˜)

### âš¡ æ€§èƒ½ä¸ä¼˜åŒ–
(åˆ†ææˆ–æ¨æµ‹ï¼š
- æ€§èƒ½ç“¶é¢ˆåœ¨å“ªï¼Ÿ
- å¦‚ä½•æ‰©å±•åˆ°ç”Ÿäº§ç¯å¢ƒï¼Ÿ
- èµ„æºæ¶ˆè€—ä¼°ç®—)

### ğŸ”Œ äºŒæ¬¡å¼€å‘æŒ‡å—
(å¦‚ä½•åŸºäºè¿™ä¸ªé¡¹ç›®è¿›è¡Œæ”¹é€ ï¼Ÿ
- å…³é”®çš„æ‰©å±•ç‚¹
- API æ¥å£è¯´æ˜
- å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰åŠŸèƒ½)

### â— å¸¸è§é—®é¢˜ä¸é¿å‘
(åˆ—å‡º 5+ ä¸ªå¸¸è§é—®é¢˜ï¼Œé™„è§£å†³æ–¹æ¡ˆ)

### ğŸš€ è¿›é˜¶å­¦ä¹ è·¯å¾„
(å­¦å®Œè¿™ä¸ªåçš„ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿç›¸å…³é¡¹ç›®æ¨è)
"""

        try:
            # ä½¿ç”¨å¼ºåŠ›æ¨¡å‹ (Analyzer/80B) ç”Ÿæˆé«˜è´¨é‡æ•™ç¨‹
            response = self.client.chat.completions.create(
                model=MODELS["analyzer"], # Switch to Strongest Model
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=3000
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"[Content] Error generating tutorial for {project['name']}: {e}")
            return f"# {project['name']}\n\n{project.get('description', 'æš‚æ— æ•™ç¨‹')}"
    
    def compare_projects(self, projects: List[Dict]) -> str:
        """å¯¹æ¯”å¤šä¸ªåŒç±»é¡¹ç›®"""
        
        project_list = "\n".join([
            f"- {p['name']}: {p.get('description', '')} (â­ {p['stars']})"
            for p in projects[:5]
        ])
        
        prompt = f"""è¯·å¯¹æ¯”ä»¥ä¸‹åŒç±»å¼€æºé¡¹ç›®ï¼Œåˆ†æå„è‡ªçš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯ï¼š

{project_list}

è¯·ç”¨è¡¨æ ¼å½¢å¼å¯¹æ¯”ï¼ŒåŒ…æ‹¬ï¼šåŠŸèƒ½ç‰¹ç‚¹ã€æ€§èƒ½ã€æ˜“ç”¨æ€§ã€ç¤¾åŒºæ´»è·ƒåº¦ã€é€‚åˆçš„ä½¿ç”¨åœºæ™¯ã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=MODELS["analyzer"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
                max_tokens=2000
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"[Content] Error comparing projects: {e}")
            return "å¯¹æ¯”ç”Ÿæˆå¤±è´¥"

    def recommend_solution(self, query: str, search_results: List[Dict]) -> str:
        """æ ¹æ®æœç´¢ç»“æœæ¨èè§£å†³æ–¹æ¡ˆ"""
        
        context = "\n".join([
            f"{i+1}. {p['name']} (â­ {p['stars']}):\n   Summary: {p.get('ai_rag_summary') or p.get('description', '')}\n   Tags: {', '.join(p.get('topics', [])[:5])}"
            for i, p in enumerate(search_results[:8])
        ])
        
        prompt = f"""ç”¨æˆ·æ­£åœ¨å¯»æ‰¾è§£å†³ä»¥ä¸‹é—®é¢˜çš„æ–¹æ¡ˆï¼š
"{query}"

åŸºäºå·²æœ‰çš„ GitHub é¡¹ç›®åº“ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³é¡¹ç›®ï¼š
{context}

è¯·æ‰®æ¼”ä¸€ä½èµ„æ·±æ¶æ„å¸ˆï¼Œä¸ºç”¨æˆ·æ¨èæœ€ä½³çš„æŠ€æœ¯é€‰å‹æ–¹æ¡ˆã€‚
å›ç­”è¦æ±‚ï¼š
1. **ç›´æ¥æ¨è**ï¼šæœ€æ¨èå“ªä¸ªé¡¹ç›®ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
2. **æ–¹æ¡ˆå¯¹æ¯”**ï¼šå¦‚æœè¿˜æœ‰å…¶ä»–é€‰æ‹©ï¼Œå®ƒä»¬çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
3. **å®æ–½å»ºè®®**ï¼šå¦‚ä½•ç»„åˆä½¿ç”¨è¿™äº›å·¥å…·ï¼Ÿ
4. **ä¸è¶³ä¹‹å¤„**ï¼šå¦‚æœæ²¡æœ‰å®Œç¾çš„åŒ¹é…ï¼Œç›®å‰æ–¹æ¡ˆçš„å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿ

è¯·ä¿æŒå®¢è§‚ã€ä¸“ä¸šï¼Œè¯­æ°”äº²åˆ‡ã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=MODELS["analyzer"], # Use Strong Model
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
                max_tokens=1500
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"[Content] Error recommending solution: {e}")
            return "æ¨èç”Ÿæˆå¤±è´¥"
