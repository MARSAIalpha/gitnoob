# GitHub Hub - Master Agent (ä»»åŠ¡è°ƒåº¦)
import threading
import time
from datetime import datetime
from typing import Dict, Callable
from database import Database
from crawler import CrawlerAgent
from analyzer import AnalyzerAgent, ContentAgent
from config import CATEGORIES, DISCOVERY_URLS

class MasterAgent:
    """ä¸»æ§ Agent - è°ƒåº¦æ‰€æœ‰å­ä»»åŠ¡"""
    
    def __init__(self, db_path: str = None):
        # db_path is now ignored - Database class uses Supabase
        self.db = Database(db_path)
        self.crawler = CrawlerAgent()
        self.analyzer = AnalyzerAgent()
        self.content = ContentAgent()
        self.is_running = False
        self.current_task = None
        self.progress = {"total": 0, "done": 0, "current": ""}
        self.callbacks = []
        self.auto_analysis_timer = None
        
        # Start auto-analysis scheduler
        self.start_auto_analysis_scheduler()

        # Ensure default news sources exist
        self._ensure_news_sources()
        
    def _ensure_news_sources(self):
        """å¦‚æœæ–°é—»æºä¸ºç©ºï¼Œåˆ™åŠ è½½é»˜è®¤æ–°é—»æº"""
        try:
            sources = self.db.get_news_sources()
            if not sources:
                print("Initializing default news sources...")
                for url in DISCOVERY_URLS:
                    name = "GitHub Trending" if "trending" in url else "News Source"
                    if "since=weekly" in url:
                        name += " (Weekly)"
                    self.db.add_news_source(name, url)
        except Exception as e:
            print(f"Error initializing sources: {e}")
    
    def start_auto_analysis_scheduler(self):
        """å¯åŠ¨åå°è‡ªåŠ¨åˆ†æè°ƒåº¦å™¨ï¼Œæ¯ 10 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡"""
        def check_and_analyze():
            while True:
                time.sleep(600)  # Wait 10 minutes
                if not self.is_running:
                    pending_count = self.db.get_pending_count()
                    if pending_count > 0:
                        self._notify(f"ğŸ¤– Auto-analysis starting: {pending_count} projects pending...", "info")
                        threading.Thread(target=self.run_batch_analysis, daemon=True).start()
        
        self.auto_analysis_timer = threading.Thread(target=check_and_analyze, daemon=True)
        self.auto_analysis_timer.start()
    
    def add_callback(self, callback: Callable):
        """æ·»åŠ è¿›åº¦å›è°ƒ"""
        self.callbacks.append(callback)
    
    def _notify(self, message: str, level: str = "info"):
        """é€šçŸ¥æ‰€æœ‰å›è°ƒ"""
        for cb in self.callbacks:
            try:
                cb({"message": message, "level": level, "time": datetime.now().isoformat()})
            except:
                pass

    def stop_task(self):
        """åœæ­¢å½“å‰ä»»åŠ¡"""
        if self.is_running:
            self.is_running = False
            self._notify("ğŸ›‘ Update: Stopping current task... (Waiting for current step to finish)", "warning")
            return {"status": "stopping"}
        return {"status": "not_running"}
    
    def run_full_scan(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´æ‰«ææµç¨‹"""
        if self.is_running:
            return {"error": "Scan already running"}
        
        self.is_running = True
        self.current_task = "full_scan"
        results = {"crawl": {}, "analyze": 0, "content": 0}
        
        try:
            # Step 0: æ¯æ—¥æ–°é—»å‘ç° (News Discovery)
            self.run_news_scan()

            # Step 1: çˆ¬å–æ‰€æœ‰åˆ†ç±» (ä¼˜å…ˆçˆ¬å–é¡¹ç›®å°‘çš„åˆ†ç±»)
            self._notify("Starting full scan (Priority: Low Count First)...", "info")
            self.progress = {"total": len(CATEGORIES), "done": 0, "current": "Crawling"}
            
            # è·å–å½“å‰å„åˆ†ç±»æ•°é‡
            cat_counts = self.db.get_all_categories_summary()
            
            # æ’åºï¼šæ•°é‡å°‘çš„æ’å‰é¢
            sorted_cats = []
            for cat_id, cat_config in CATEGORIES.items():
                count = cat_counts.get(cat_id, {}).get('count', 0)
                sorted_cats.append((cat_id, cat_config, count))
            
            sorted_cats.sort(key=lambda x: x[2])
            
            for cat_id, cat_config, count in sorted_cats:
                self.progress["current"] = f"Crawling: {cat_config['name']} (Current: {count})"
                self._notify(f"Scanning {cat_config['name']} (Current: {count})...", "info")
                
                if cat_id == "trending":
                    projects = self.crawler.get_trending()
                elif cat_id == "new_releases":
                    projects = self.crawler.get_new_releases()
                else:
                    projects = self.crawler.search_by_keywords(cat_config["keywords"], cat_id)
                
                for project in projects:
                    self.db.upsert_project(project)
                
                results["crawl"][cat_id] = len(projects)
                self.progress["done"] += 1
                self._notify(f"Found {len(projects)} in {cat_config['name']}", "success")
            
            # Step 2: AI åˆ†ææœªå¤„ç†çš„é¡¹ç›®
            self._notify("Starting AI analysis...", "info")
            pending = self.db.get_projects_needing_analysis(limit=50)
            self.progress = {"total": len(pending), "done": 0, "current": "Analyzing"}
            
            for project in pending:
                self.progress["current"] = f"Analyzing: {project['name']}"
                
                # è·å– README
                readme = self.crawler.get_readme(project['full_name'])
                
                # AI åˆ†æ
                analysis = self.analyzer.analyze_project(project, readme)
                self.db.update_ai_analysis(project['id'], analysis)
                
                results["analyze"] += 1
                self.progress["done"] += 1
                self._notify(f"Analyzed: {project['name']}", "success")
                
                # é¿å… API é™åˆ¶
                time.sleep(0.5)
            
            self._notify("Full scan completed!", "success")
            
            # Step 3: è‡ªåŠ¨å½’æ¡£æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶å¤¹
            self.archive_data()
            
        except Exception as e:
            self._notify(f"Error during scan: {e}", "error")
            results["error"] = str(e)
        finally:
            self.is_running = False
            self.current_task = None
        
        return results
    
    def archive_data(self):
        """å½’æ¡£æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶å¤¹"""
        import os
        import json
        
        date_str = datetime.now().strftime("%Y-%m-%d")
        archive_dir = f"data/archive/{date_str}"
        os.makedirs(archive_dir, exist_ok=True)
        
        self._notify(f"Archiving data to {archive_dir}...", "info")
        
        try:
            # 1. å¯¼å‡ºæ‰€æœ‰æ•°æ®
            all_projects = self.db.get_projects_by_category("all", limit=10000) # Hack to get all? No, need new DB method or iterate categories
            # Better: iterate categories
            
            summary_stats = {"total": 0, "breakdown": {}}
            
            for cat_id, cat_config in CATEGORIES.items():
                projects = self.db.get_projects_by_category(cat_id, limit=1000)
                if not projects:
                    continue
                    
                # ä¿å­˜æ¯ä¸ªåˆ†ç±»çš„ JSON
                file_path = f"{archive_dir}/{cat_id}.json"
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump([dict(p) for p in projects], f, ensure_ascii=False, indent=2)
                
                summary_stats["breakdown"][cat_id] = len(projects)
                summary_stats["total"] += len(projects)
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            with open(f"{archive_dir}/_stats.json", 'w', encoding='utf-8') as f:
                json.dump(summary_stats, f, ensure_ascii=False, indent=2)
                
            self._notify(f"Data archived successfully to {archive_dir}", "success")
            
        except Exception as e:
            print(f"Archive error: {e}")
            self._notify(f"Archive error: {e}", "error")

    def run_batch_analysis(self) -> Dict:
        """æ‰¹é‡åˆ†ææ‰€æœ‰æœªåˆ†æçš„é¡¹ç›® (ä½¿ç”¨å¿«é€Ÿæ¨¡å‹)"""
        if self.is_running:
            return {"error": "Task already running"}
            
        self.is_running = True
        self.current_task = "batch_analysis"
        
        try:
            # è·å–æ‰€æœ‰é¡¹ç›®ï¼ˆå¯ä»¥ç¨å¾®ä¼˜åŒ–åªè·å–æœªåˆ†æçš„ï¼Œä½†ä¸ºäº†æ¼”ç¤ºæ•ˆæœï¼Œæˆ‘ä»¬å¯ä»¥éå†æ‰€æœ‰æ²¡æœ‰ tutorial çš„ï¼‰
            # æˆ–è€…åªè·å–å‰ 50 ä¸ªæœªåˆ†æçš„
            pending = self.db.get_projects_needing_analysis(limit=100) # Increased limit
            
            self._notify(f"Starting batch analysis for {len(pending)} projects using Deepseek 8B...", "info")
            self.progress = {"total": len(pending), "done": 0, "current": "Batch Analysis"}
            
            analyzed_count = 0
            total = len(pending)
            
            for idx, project in enumerate(pending, 1):
                if not self.is_running: break # Allow cancellation (not implemented yet but good practice)
                
                progress_tag = f"[{idx}/{total}]"
                self.progress["current"] = f"Generating Tutorial: {project['name']}"
                self._notify(f"{progress_tag} æ­£åœ¨åˆ†æ {project['name']}...", "info")
                
                # å§‹ç»ˆè·å– README (æ•™ç¨‹ç”Ÿæˆéœ€è¦)
                readme = self.crawler.get_readme(project['full_name'])
                
                # 1. ç”Ÿæˆ/æ›´æ–° Analysis (å¦‚æœç¼ºå¤±)
                if not project.get('ai_summary'):
                    analysis = self.analyzer.analyze_project(project, readme)
                    self.db.update_ai_analysis(project['id'], analysis)
                else:
                    analysis = {"summary": project.get('ai_summary')}
                
                if not project.get('ai_summary'):
                    analysis = self.analyzer.analyze_project(project, readme)
                    self.db.update_ai_analysis(project['id'], analysis)
                else:
                    analysis = {"summary": project.get('ai_summary')}
                
                # 1.1 ç”Ÿæˆ RAG Summary (å¦‚æœç¼ºå¤±)
                if not project.get('ai_rag_summary'):
                    rag_summary = self.analyzer.generate_rag_summary(project, readme)
                    if rag_summary:
                        with self.db.lock:
                            self.db.conn.execute("UPDATE projects SET ai_rag_summary = ? WHERE id = ?", (rag_summary, project['id']))
                            self.db.conn.commit()
                
                # 2. æŠ“å–æˆªå›¾ (å¦‚æœç¼ºå¤± æˆ– å¼ºåˆ¶æ›´æ–°)
                screenshot_path = project.get('screenshot')
                if not screenshot_path:
                    self._notify(f"{progress_tag} æ­£åœ¨æˆªå›¾ {project['name']}...", "info")
                    screenshot_path = self.crawler.capture_screenshot(project['url'], project['id'])
                    if screenshot_path:
                        self.db.update_screenshot(project['id'], screenshot_path)
                
                # 3. è§†è§‰åˆ†æ (OCR & UI)
                visual_summary = ""
                if screenshot_path:
                    self._notify(f"{progress_tag} è§†è§‰åˆ†æ (OCR) {project['name']}...", "info")
                    visual_summary = self.analyzer.analyze_with_vision(project, screenshot_path)
                    # Update DB with visual summary (need update method, handled in update_ai_analysis context or separate)
                    # Actually update_ai_analysis handles it if we pass it, but here we might want to attach it to 'analysis' dict
                    # Let's trust generate_tutorial uses it, and we save it too.
                    # We updated update_ai_analysis to take visual_summary key
                    if not analysis.get('visual_summary'):
                        analysis['visual_summary'] = visual_summary
                        self.db.update_ai_analysis(project['id'], analysis)

                # 4. ç”Ÿæˆæ•™ç¨‹ (ä½¿ç”¨å¼ºåŠ›æ¨¡å‹ + è§†è§‰ä¿¡æ¯)
                self._notify(f"{progress_tag} ç”Ÿæˆæ·±åº¦æ•™ç¨‹ (80B)...", "info")
                # Need to update generate_tutorial signature in master logic or calling
                # master.generate_tutorial calls self.content.generate_tutorial
                # Let's call content agent directly or update master.generate_tutorial signature?
                # Let's update master's generate_tutorial to accept visual_summary
                tutorial = self.content.generate_tutorial(project, readme, visual_summary)
                
                # Update DB
                with self.db.lock:
                    self.db.conn.execute("UPDATE projects SET ai_tutorial = ? WHERE id = ?", (tutorial, project['id']))
                    self.db.conn.commit()
                
                analyzed_count += 1
                self.progress["done"] += 1
                self._notify(f"{progress_tag} âœ… {project['name']} æ•™ç¨‹ç”Ÿæˆå®Œæˆ", "success")
                
                # ç¨å¾®å»¶æ—¶
                time.sleep(1)
            
            self._notify(f"ğŸ‰ æ‰¹é‡åˆ†æå®Œæˆï¼å…±å¤„ç† {analyzed_count} ä¸ªé¡¹ç›®", "success")
            return {"status": "completed", "count": analyzed_count}
            
        except Exception as e:
            self._notify(f"Batch analysis error: {e}", "error")
            return {"error": str(e)}
        finally:
            self.is_running = False
            self.current_task = None

    def run_category_scan(self, category: str) -> Dict:
        """æ‰«æå•ä¸ªåˆ†ç±»"""
        if category not in CATEGORIES:
            return {"error": f"Unknown category: {category}"}
        
        cat_config = CATEGORIES[category]
        self._notify(f"Scanning {cat_config['name']}...", "info")
        
        if category == "trending":
            projects = self.crawler.get_trending()
        elif category == "new_releases":
            projects = self.crawler.get_new_releases()
        else:
            projects = self.crawler.search_by_keywords(cat_config["keywords"], category)
        
        for project in projects:
            self.db.upsert_project(project)
        
        self._notify(f"Found {len(projects)} projects", "success")
        return {"category": category, "count": len(projects)}
    
    def run_news_scan(self) -> Dict:
        """æ‰«ææ¯æ—¥å‘ç°æº (News Discovery)"""
        self.is_running = True
        self.current_task = "news_scan"
        results = {"total_found": 0, "sources": {}}
        
        try:
            self._notify("Starting News Discovery Scan...", "info")
            total_found = 0
            
            for url in DISCOVERY_URLS:
                self._notify(f"Crawling source: {url}...", "info")
                projects = self.crawler.crawl_external_page(url)
                
                # Assign to 'news' category if generic, or try to auto-classify later?
                # For now let's put them in 'news' category or 'manual'
                # The user asked for "continuous discovery", so maybe a 'news' category is best.
                # But our frontend needs to support 'news' category ID if we use it.
                # Let's check config.py... yes, 'news' category exists!
                
                source_count = 0
                for p in projects:
                    # å¦‚æœåº“é‡Œæ²¡æœ‰ï¼Œæ‰ç®—æ–°å‘ç°
                    if not self.db.get_project(p['id']):
                        p['category'] = 'news' # Force category
                        self.db.upsert_project(p)
                        source_count += 1
                        self._notify(f"Found new project: {p['name']}", "success")
                
                results["sources"][url] = source_count
                total_found += source_count
                
            self._notify(f"News Scan Completed. Found {total_found} new projects.", "success")
            results["total_found"] = total_found
            return results
            
        except Exception as e:
            self._notify(f"News scan error: {e}", "error")
            return {"error": str(e)}
        finally:
            self.is_running = False
            self.current_task = None

    def add_project_by_link(self, url: str) -> Dict:
        """ä»é“¾æ¥æ·»åŠ é¡¹ç›®"""
        self._notify(f"Fetching project from {url}...", "info")
        project = self.crawler.fetch_project_by_url(url)
        
        if not project:
            return {"error": "Failed to fetch project. Check URL or network."}
        
        self.db.upsert_project(project)
        self._notify(f"Added {project['name']}. Starting analysis...", "success")
        
        # ç«‹å³åˆ†æ
        threading.Thread(target=self.analyze_single, args=(project['id'],)).start()
        
        return {"status": "added", "project": project['name']}

    def reset_all_data(self) -> Dict:
        """é‡ç½®æ‰€æœ‰æ•°æ®"""
        try:
            self.db.clear_database()
            self._notify("Database cleared. All project data removed.", "warning")
            return {"status": "success", "message": "Database reset successfully"}
        except Exception as e:
            return {"error": str(e)}

    def analyze_single(self, project_id: str) -> Dict:
        """åˆ†æå•ä¸ªé¡¹ç›®"""
        cursor = self.db.conn.execute(
            "SELECT * FROM projects WHERE id = ?", (project_id,)
        )
        row = cursor.fetchone()
        if not row:
            return {"error": "Project not found"}
        
        project = dict(row)
        
        # æŠ“å–æˆªå›¾
        if not project.get('screenshot'):
            screenshot_path = self.crawler.capture_screenshot(project['url'], project_id)
            if screenshot_path:
                self.db.update_screenshot(project_id, screenshot_path)
        
        readme = self.crawler.get_readme(project['full_name'])
        analysis = self.analyzer.analyze_project(project, readme)
        self.db.update_ai_analysis(project_id, analysis)
        
        # Generate RAG Summary
        rag_summary = self.analyzer.generate_rag_summary(project, readme)
        self.db.conn.execute("UPDATE projects SET ai_rag_summary = ? WHERE id = ?", (rag_summary, project_id))
        self.db.conn.commit()
        
        return analysis
    
    def generate_tutorial(self, project_id: str) -> str:
        """ç”Ÿæˆé¡¹ç›®æ•™ç¨‹"""
        cursor = self.db.conn.execute(
            "SELECT * FROM projects WHERE id = ?", (project_id,)
        )
        row = cursor.fetchone()
        if not row:
            return "Project not found"
        
        project = dict(row)
        readme = self.crawler.get_readme(project['full_name'])
        tutorial = self.content.generate_tutorial(project, readme)
        
        # ä¿å­˜æ•™ç¨‹
        self.db.conn.execute(
            "UPDATE projects SET ai_tutorial = ? WHERE id = ?",
            (tutorial, project_id)
        )
        self.db.conn.commit()
        
        # åŒæ—¶ä¿å­˜ä¸º Markdown æ–‡ä»¶
        try:
            timestamp = datetime.now().strftime("%Y%m%d")
            filename = f"data/tutorials/{project['name']}_{timestamp}.md"
            import os
            os.makedirs("data/tutorials", exist_ok=True)
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(tutorial)
        except:
            pass
        
        return tutorial
    
    def get_status(self) -> Dict:
        """è·å–å½“å‰çŠ¶æ€"""
        return {
            "is_running": self.is_running,
            "current_task": self.current_task,
            "progress": self.progress,
            "categories": self.db.get_all_categories_summary()
        }
    
    
    def search_hybrid(self, query: str, limit: int = 20) -> Dict:
        """æ··åˆæœç´¢ï¼šæœ¬åœ° DB + GitHub å®æ—¶æœç´¢"""
        results = {"local": [], "remote": []}
        
        # 1. æœ¬åœ°æœç´¢
        local_results = self.db.search_projects(query, limit=limit)
        results["local"] = local_results
        
        # 2. å¦‚æœæœ¬åœ°ç»“æœå¾ˆå°‘ï¼Œæˆ–è€…å¼ºåˆ¶æ··åˆï¼Œåˆ™æœç´¢ GitHub
        # ç­–ç•¥ï¼šæœ¬åœ°ä¸è¶³ 5 ä¸ªï¼Œæˆ–è€…ç”¨æˆ·æ˜¾å¼è¦æ±‚æ­¤åŠŸèƒ½
        if len(local_results) < 5:
            # å¯åŠ¨ GitHub æœç´¢ (æ³¨æ„è¿™æ˜¯ä¸€ä¸ªé˜»å¡æ“ä½œï¼Œè€—æ—¶çº¦ 1-2s)
            remote_results = self.crawler.search_remote(query, limit=10)
            
            # å»é‡ï¼šè¿‡æ»¤æ‰å·²ç»åœ¨æœ¬åœ°ç»“æœä¸­çš„é¡¹ç›®
            local_ids = {str(p['id']) for p in local_results}
            for p in remote_results:
                if str(p['id']) not in local_ids:
                    results["remote"].append(p)
                    
        return results

    def close(self):
        self.db.close()


def run_scheduled_scan(master: MasterAgent):
    """å®šæ—¶æ‰«æä»»åŠ¡ (åå°çº¿ç¨‹)"""
    print("[Scheduler] Scheduler started.")
    
    last_run_date = None
    
    while True:
        try:
            now = datetime.now()
            # è·å–è®¾ç½®çš„æ—¶é—´ï¼Œé»˜è®¤ 02:00
            scan_time_str = master.db.get_setting("scan_time", "02:00")
            try:
                target_hour, target_minute = map(int, scan_time_str.split(':'))
            except:
                target_hour, target_minute = 2, 0
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æ—¶é—´ä¸”ä»Šå¤©æœªè¿è¡Œ
            if now.hour == target_hour and now.minute == target_minute:
                today_str = now.strftime("%Y-%m-%d")
                if last_run_date != today_str:
                    print(f"[Scheduler] Starting scheduled scan at {scan_time_str}...")
                    master.run_full_scan()
                    last_run_date = today_str
                    time.sleep(65) # é¿å…è¿™ä¸€åˆ†é’Ÿå†…é‡å¤è§¦å‘
            
            time.sleep(30)
        except Exception as e:
            print(f"[Scheduler] Error: {e}")
            time.sleep(60)
